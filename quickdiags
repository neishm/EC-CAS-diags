#!/usr/bin/env python

# Model parameters to assume for this data:
tracers = ('CO2','CH4','TCO')
nlev = 79   # Number of thermodynamic levels
debug = False
grav = .980616e+1  # Taken from GEM-MACH file chm_consphychm_mod.ftn90

from argparse import ArgumentParser
parser = ArgumentParser(description='Performs some quick diagnostics on EC-CAS model output.')
parser.add_argument('infile', help='Input file or directory to process.')
parser.add_argument('outdir', nargs='?', default='.', help='Where to put the output files.  Default is the current directory.')
parser.add_argument('--label', help='The name of the experiment.  If not specified, then a name will be determined from the input directory structure.')
args = parser.parse_args()

try:
  import fstd2nc
  import rpnpy.librmn.all as rmn
  import rpnpy.vgd.all as vgd
except ImportError:
  parser.error("You need to run the following command before using the script:\n\n. ssmuse-sh -p eccc/crd/ccmr/EC-CAS/master/fstd2nc_0.20180821.0\n")

from os.path import dirname
if args.label is None:
  dirs = dirname(args.infile).split('/')
  while dirs[-1] in ('model','pressure','timeseries','','.'):
    dirs.pop()
  args.label = dirs[-1]

import netCDF4

# Helper functions

# Class for handling dependencies on calculations.
class Calc (object):
  __slots__ = ('func','inputs','callbacks','__weakref__')
  def __init__ (self, func, *inputs):
    import weakref
    self.func = func
    self.inputs = list(inputs)
    for inp in inputs:
      if isinstance(inp,Calc):
        if debug: print '%s will call back %s'%(inp,func.func_name)
        inp.callbacks.add(self)
    self.callbacks = weakref.WeakSet()
  def trigger (self):
    if debug: print 'trigger %s? %s'%(self.func.func_name, [not isinstance(inp,Calc) for inp in self.inputs])
    if not any(isinstance(inp,Calc) for inp in self.inputs):
      if debug: print 'triggered'
      data = self.func(*self.inputs)
      self._do_callback (data)
    else:
      if debug: print 'not triggered'
  def _do_callback (self, data):
    if debug: print 'callbacks:', list(self.callbacks)
    for callback in self.callbacks:
      ids = list(map(id,callback.inputs))
      if id(self) not in ids:
        print 'weird'
        return
      i = ids.index(id(self))
      callback.inputs[i] = data
      callback.trigger()
  def __del__ (self):
    if debug: print 'deleting %s'%(self.func.func_name)

# Decorate for turning a regular function into one that handles dependent
# calculations.
import weakref
def calc (func, lookup=weakref.WeakValueDictionary()):
  from functools import wraps
  @wraps(func)
  def f (*args):
    key = (func,tuple(args))
    if debug and key in lookup:
      print 'reusing existing %s (%s)'%(func.func_name, ','.join(map(str,args)))
    elif debug:
      print 'generating new %s (%s)'%(func.func_name, ','.join(map(str,args)))
    result = lookup.get(key,None)
    if result is None:
      result = Calc(func,*args)
    lookup[key] = result
    return result
  return f
del weakref

# Keep track of all records that are used.
all_records = set()

@calc
def _read (rec_id):
  if debug: print 'read'
  all_records.remove(rec_id)
  return np.asarray(fstluk(rec_id),dtype='float64')
def read (rec_id):
  all_records.add(rec_id)
  return _read (rec_id)


# Object for managing writes into a netCDF4 dataset.
class Writer (Calc):
  def __init__ (self, dataset):
    Calc.__init__(self,None)
    self.dataset = dataset
  def trigger (self):
    if not any(isinstance(inp,Calc) for inp in self.inputs):
      if debug: print 'close file'
      self.dataset.close()
  def write (self, varname, ind, data):
    if not isinstance(data,Calc):
      self.dataset[varname][ind] = data
      return
    inp = _write (self.dataset[varname], ind, data)
    self.inputs.append(inp)
    inp.callbacks.add(self)
  def __del__ (self):
    return

# Object for doing an individual write.
@calc
def _write (var, ind, data):
  if debug: print 'write'
  var[ind] = data


@calc
def logp (p0):
  if debug: print 'logp'
  import numpy as np
  return np.log(p0/1000.)

@calc
def pres (a, b, s):
  if debug: print 'pres'
  import numpy as np
  return np.exp(a+b*s)

@calc
def layer_mass (c,q,p_above,p_below):
  if debug: print 'layer mass'
  return c*(1-q)*(p_below-p_above)

@calc
def area_integrate (x, dx, scale):
  if debug: print 'area integrate'
  import numpy as np
  # Remove repeated longitude
  x = x[:,:-1]
  dx = dx[:,:-1]
  return np.sum(x*dx*scale)

@calc
def layer_sum (*inputs):
  if debug: print 'layer sum'
  return sum(inputs)


import numpy as np

fstd2nc.stdout.streams = ('error',)
if not debug:
  rmn.fstopt('MSGLVL','ERRORS')
b = fstd2nc.Buffer(args.infile, ignore_diag_level=True, rpnstd_metadata=True, progress=True, unique_names=False)

# Call c_fstluk directly
all_keys = np.array(b._headers['key'])
all_ni = np.array(b._headers['ni'])
all_nj = np.array(b._headers['nj'])
all_nk = np.array(b._headers['nk'])
all_shape = list(zip(all_ni,all_nj,all_nk))
all_datyp = np.array(b._headers['datyp'])
all_nbits = np.array(b._headers['nbits'])
all_file_id = np.array(b._headers['file_id'])

def fstluk (rec_id):
  import ctypes as _ct
  import numpy.ctypeslib as _npc
  from rpnpy.librmn import proto as _rp
  from rpnpy.librmn.fstd98 import dtype_fst2numpy
  import numpy as np
  file_id = all_file_id[rec_id]
  b._open(file_id)
  key = (all_keys[rec_id]<<10) + b._opened_librmn_index
  shape = all_shape[rec_id]
  dtype = dtype_fst2numpy(int(all_datyp[rec_id]),int(all_nbits[rec_id]))
  _rp.c_fstluk.argtypes = (_npc.ndpointer(dtype=dtype), _ct.c_int,
                           _ct.POINTER(_ct.c_int), _ct.POINTER(_ct.c_int),
                           _ct.POINTER(_ct.c_int))
  data = np.empty(shape, dtype=dtype, order='FORTRAN')
  (cni, cnj, cnk) = (_ct.c_int(), _ct.c_int(), _ct.c_int())
  istat = _rp.c_fstluk(data, key, _ct.byref(cni), _ct.byref(cnj),
                       _ct.byref(cnk))
  if istat < 0:
    raise FSTDError()
  return data.transpose().squeeze()

# Get vertical parameters for this data.
vgd_id = vgd.vgd_read(b._opened_funit)
am = vgd.vgd_get(vgd_id,'CA_M')[:-1]
bm = vgd.vgd_get(vgd_id,'CB_M')[:-1]
at = vgd.vgd_get(vgd_id,'CA_T')[:-2]
bt = vgd.vgd_get(vgd_id,'CB_T')[:-2]
assert len(am) == nlev+1
assert len(at) == nlev
assert np.all(am[:-1] < at)
assert np.all(am[1:] > at)

# Structure the records into time/level dimensions.
b._makevars()
vardict = {v.name:v for v in b._varlist}

# Get grid cell areas.
dx = read(vardict['DX'].record_id[0])

# Select records with full vertical extent
def get_3d_recs (recs):
  return recs[np.all(recs>=0, axis=1),:]

# Get unique timesteps from records
def get_timesteps (recs):
  if recs.ndim > 1:
    recs = recs[:,0]
  return set(b._headers[recs]['datev'])

# Limit records to those that fall on particular timesteps
def limit_timesteps (recs, target_timesteps):
  if recs.ndim > 1:
    timesteps = b._headers[recs[:,0]]['datev']
  else:
    timesteps = b._headers[recs]['datev']
  return recs[np.isin(timesteps,list(target_timesteps))]

##############################################################################
# Mass diagnostics

# Find timesteps available for mass diagnostics.
mass_timesteps = get_timesteps(get_3d_recs(vardict['HU'].record_id)) \
               & get_timesteps(vardict['P0'].record_id)

HU_recs = limit_timesteps(get_3d_recs(vardict['HU'].record_id), mass_timesteps)
P0_recs = limit_timesteps(vardict['P0'].record_id, mass_timesteps)

# Find the available timesteps and levels.
mass_timesteps = b._headers[HU_recs[:,0]]['datev']
mass_levels = b._headers[HU_recs[0,:]]['level']
assert np.all(vgd.vgd_get(vgd_id,'VIPT')[:-2] == b._headers[HU_recs[0,:]]['ip1'])

# Determine mass calculations.
filename = "%s/%s_totalmass.nc"%(args.outdir,args.label)
tropo_filename = "%s/%s_tropomass.nc"%(args.outdir,args.label)
totalmass = netCDF4.Dataset(filename,'w')
tropo_totalmass = netCDF4.Dataset(tropo_filename,'w')
times = fstd2nc.mixins.dates.stamp2datetime(mass_timesteps)
times = np.array(times,dtype='datetime64[s]')
for dataset in totalmass, tropo_totalmass:
  dataset.createDimension('time', None)
  dataset.createVariable(varname='time', datatype='float32', dimensions=('time',))
  dataset['time'].setncatts({'units':'hours since 2015-01-01'})
  dataset['time'][:] = netCDF4.date2num(times.tolist(),units='hours since 2015-01-01')
  for tracer in tracers:
    dataset.createVariable(varname=tracer, datatype='float64', dimensions=('time',))
  dataset.createVariable(varname='air', datatype='float64', dimensions=('time',))
  dataset.createVariable(varname='dry_air', datatype='float64', dimensions=('time',))

totalmass = Writer(totalmass)
tropo_totalmass = Writer(tropo_totalmass)

for tracer in tracers:
  recs = limit_timesteps(get_3d_recs(vardict[tracer].record_id), mass_timesteps)
  for it,t in enumerate(np.searchsorted(mass_timesteps, b._headers[recs[:,0]]['datev'])):
    layers = []
    tropo_layers = []
    for ik,k in enumerate(np.searchsorted(mass_levels, b._headers[recs[0,:]]['level'])):
      c = read(recs[it,ik])
      q = read(HU_recs[t,k])
      s = logp(read(P0_recs[t]))
      p_above = pres(am[k],bm[k],s)
      p_below = pres(am[k+1],bm[k+1],s)
      layer = layer_mass(c,q,p_above,p_below)
      layers.append(layer)
      if mass_levels[k] > 0.2:
        tropo_layers.append(layer)
    mass = layer_sum(*layers)
    mass = area_integrate(mass, dx, 1E-9/grav*1E-12)
    totalmass.write (tracer, t, mass)
    mass = layer_sum(*tropo_layers)
    mass = area_integrate(mass, dx, 1E-9/grav*1E-12)
    tropo_totalmass.write (tracer, t, mass)
# Dry-air mass
for t in range(len(mass_timesteps)):
  layers = []
  tropo_layers = []
  for k in range(len(mass_levels)):
    q = read(HU_recs[t,k])
    s = logp(read(P0_recs[t]))
    p_above = pres(am[k],bm[k],s)
    p_below = pres(am[k+1],bm[k+1],s)
    layer = layer_mass(1,q,p_above,p_below)
    layers.append(layer)
    if mass_levels[k] > 0.2:
      tropo_layers.append(layer)
  mass = layer_sum(*layers)
  mass = area_integrate(mass, dx, 1/grav*1E-12)
  totalmass.write ('dry_air', t, mass)
  mass = layer_sum(*tropo_layers)
  mass = area_integrate(mass, dx, 1/grav*1E-12)
  tropo_totalmass.write ('dry_air', t, mass)
# Total air mass
for t in range(len(mass_timesteps)):
  s = logp(read(P0_recs[t]))
  p_above = pres(am[0],bm[0],s)
  p_below = pres(am[nlev],bm[nlev],s)
  mass = layer_mass(1,0,p_above,p_below)
  mass = area_integrate(mass, dx, 1/grav*1E-12)
  totalmass.write ('air', t, mass)
  mass = layer_mass(1,0,20000,p_below)
  mass = area_integrate(mass, dx, 1/grav*1E-12)
  tropo_totalmass.write ('air', t, mass)


if debug: print 'starting I/O'

# Read DX first.
# These records seem to be found at the end of the RPN file, so all the other
# operations will be waiting a long time if this isn't pre-loaded.
dx.trigger()

if not debug:
  pbar = fstd2nc.mixins._ProgressBar("Calculating diagnostics", suffix='%(percent)d%% [%(myeta)s]', max=len(all_records))

for rec_id in sorted(all_records):
  read(rec_id).trigger()
  if not debug: pbar.next()
if not debug: pbar.finish()


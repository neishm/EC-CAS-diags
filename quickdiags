#!/usr/bin/env python

# Model parameters to assume for this data:
tracers = ('CO2','CH4','TCO')
nlev = 79   # Number of thermodynamic levels
debug = False
grav = .980616e+1  # Taken from GEM-MACH file chm_consphychm_mod.ftn90

from argparse import ArgumentParser
parser = ArgumentParser(description='Performs some quick diagnostics on EC-CAS model output.')
parser.add_argument('infile', help='Input file or directory to process.')
parser.add_argument('outdir', nargs='?', default='.', help='Where to put the output files.  Default is the current directory.')
parser.add_argument('--label', help='The name of the experiment.  If not specified, then a name will be determined from the input directory structure.')
args = parser.parse_args()

try:
  import fstd2nc
  import rpnpy.librmn.all as rmn
except ImportError:
  parser.error("You need to run the following command before using the script:\n\n. ssmuse-sh -p eccc/crd/ccmr/EC-CAS/master/fstd2nc_0.20180821.0\n")

from os.path import dirname
if args.label is None:
  dirs = dirname(args.infile).split('/')
  while dirs[-1] in ('model','pressure','timeseries','','.'):
    dirs.pop()
  args.label = dirs[-1]

fstd2nc.stdout.streams = ('error',)
rmn.fstopt('MSGLVL','ERRORS')
b = fstd2nc.Buffer(args.infile, ignore_diag_level=True, rpnstd_metadata=True, progress=True, unique_names=False)


# Helper function to subset records to match HU
def subset (var, hu, b):
  import numpy as np
  datev = b._headers['datev'][hu.record_id[:,0]]
  ip1 = b._headers['ip1'][hu.record_id[0,:]]
  recs = var.record_id
  if recs.ndim == 1:
    recs = recs[np.array([d in datev for d in b._headers['datev'][recs]])]
  else:
    recs = recs[np.array([d in datev for d in b._headers['datev'][recs[:,0]]])]
    recs = recs[:,np.array([i in ip1 for i in b._headers['ip1'][recs[0,:]]])]
  var.record_id = recs

class TracerMass (object):
  name = 'tracer_mass'

  def __init__ (self, b):
    import numpy as np
    b._makevars()

    # Determine the timesteps available for mass diagnostics.
    # Assume tracers will have same output frequency as HU.
    for v in b._varlist:
      if v.name == 'HU': hu = v
      if v.name == 'P0': p0 = v
      if v.name == 'DX': dx = v

    # Get momentum-level A and B vertical parameters.
    self._am = np.array(hu.axes[1].atts['CA_M'])[:-1]
    self._bm = np.array(hu.axes[1].atts['CB_M'])[:-1]
    assert len(self._am) == nlev + 1
    subset(p0, hu, b)
    # Pre-load dx, since we only need a single copy of the record.
    self._dx = b._fstluk(dx.record_id[0])['d']

    table = []
    for v in b._varlist:
      if v.name not in tracers: continue
      if v.record_id.ndim != 2: continue
      subset(v, hu, b)
      for ind in np.ndindex(v.record_id.shape):
        t = ind[0]
        k = ind[1]
        table.append(('c',v.record_id[ind],v.name,t,k))
        table.append(('q',hu.record_id[ind],v.name,t,k))
        table.append(('p0',p0.record_id[ind[0]],v.name,t,k))

    self.table = table
    self.axes = (hu.axes[0],)

  # The function that will be called to compute the mass.
  def calc (self, c, q, p0, ind, loop, out=None):
    import numpy as np
    am = self._am
    bm = self._bm
    dx = self._dx
    # Ignore repeated longitude
    c = c[:,:-1]
    q = q[:,:-1]
    p0 = p0[:,:-1]
    dx = dx[:,:-1]
    if out is None:
      out = np.zeros((),dtype=np.float64)
    k = loop
    plog = np.log(p0/1000.)
    p_above = np.exp(am[k] + bm[k]*plog)
    p_below = np.exp(am[k+1] + bm[k+1]*plog)
    dp = p_below - p_above
    out[...] += np.sum(c*(1-q)*dp*dx) * 1E-9 / grav * 1E-12
    return out

diags = [TracerMass(b)]


###############################################################################
# Do the diagnostics
import pandas as pd
import netCDF4
from fstd2nc.mixins import _ProgressBar as ProgressBar

# Helper class - readahead for records.
class ReadAhead (object):
  def __init__ (self, b, recs):
    self.b = b
    self.recs = sorted(recs)
    self._read_ahead()
  def _do_read (self, rec_id):
    self.data = self.b._fstluk(rec_id)['d']
  def _read_ahead (self):
    from threading import Thread
    if not hasattr(self,'i'):
      self.i = -1
    self.i += 1
    if self.i >= len(self.recs):
      return  # Nothing left to read ahead
    rec_id = self.recs[self.i]
    if hasattr(self,'thread'):
      self.thread.join()
    self.thread = Thread(target=self._do_read, args=[rec_id])
    self.thread.start()
  def read (self, rec_id):
    self.thread.join()
    expected = self.recs[self.i]
    assert rec_id == expected, "Expected request for record %d, but got request for %d."%(expected, rec_id)
    data = self.data
    self._read_ahead()
    return data


diags = [pd.DataFrame(d.table, columns=('parameter','record_id','field','ind','loop')).assign(diagnostic=d) for d in diags]
diags = pd.concat(diags)
readahead = ReadAhead(b, diags['record_id'].unique())


# Get list of parameters needed for each diagnostic.
params_needed = {}
outer_sizes = {}
for diagnostic, group in diags.groupby('diagnostic', group_keys=False):
  params_needed[diagnostic] = list(group['parameter'].unique())
  biggest_ind = max(group['ind'])
  if isinstance(biggest_ind,int):
    biggest_ind = (biggest_ind,)
  outer_sizes[diagnostic] = tuple([i+1 for i in biggest_ind])

loops = {}
for stuff, group in diags.groupby(['diagnostic','field','parameter','ind'], group_keys=False):
  loops[diagnostic] = len(group['loop'])

outfiles = {}  # The output files for each diagnostic.
loop_workspace = {} # The workspace for looping over records.
out_workspace = {}  # The workspace for building the final output.
loopcounts = {} # How many loop iterations have been done so far.

iterator = diags.groupby('record_id', group_keys=False)
if not debug: iterator = ProgressBar("Calculating diagnostics", suffix='%(percent)d%% [%(myeta)s]', max=len(diags['record_id'].unique())).iter(iterator)
for rec_id, group in iterator:
  if debug: print 'read %s record %d'%(b._headers['nomvar'][rec_id],rec_id)
  data = readahead.read(rec_id)
  #data = None
  for param, r, field, ind, loop, diagnostic in group.itertuples(index=False):
    if debug: print "will use as parameter '%s' for %s %s (ind=%s) (%s)"%(param, diagnostic.name, field, ind, loop)
    outkey = (diagnostic,field,ind)
    out_workspace.setdefault(outkey,None)
    loopkey = (diagnostic, field, ind, loop)
    if loopkey not in loop_workspace:
      loop_workspace[loopkey] = {}
    w = loop_workspace[loopkey]
    w[param] = data

    # All input parameters have been collected for this loop?
    if len(w) == len(params_needed[diagnostic]):
      if debug: print "calculating %s %s (ind=%s) (%s)"%(diagnostic.name, field, ind, loop)
      out = diagnostic.calc (ind=ind,loop=loop,out=out_workspace[outkey],**w)
      loop_workspace.pop(loopkey)
      out_workspace[outkey] = out  # Store for later use (if need multiple calcs per output)
      loopcounts.setdefault(outkey,0)
      loopcounts[outkey] += 1
      assert loopcounts[outkey] <= loops[diagnostic]
      # All loops finished for this output?
      if loopcounts[outkey] == loops[diagnostic]:
        #TODO: set chunk sizes
        filename = "%s/%s_%s.nc"%(args.outdir,args.label,diagnostic.name)
        if filename not in outfiles:
          if debug: print "opening file %s for writing"%(filename)
          outfiles[filename] = netCDF4.Dataset(filename,'w')
        dataset = outfiles[filename]
        dimnames = [a.name for a in diagnostic.axes]
        if len(dataset.dimensions) == 0:
          sizes = outer_sizes[diagnostic] + out.shape
          assert len(dimnames) == len(sizes)
          if debug: print "defining dimensions %s of size %s"%(dimnames, sizes)
          for dimname, size in zip(dimnames, sizes):
            dataset.createDimension(dimname, size)
        if field not in dataset.variables:
          if debug: print "adding %s to %s"%(field,filename)
          dataset.createVariable(varname=field, datatype=out.dtype, dimensions=dimnames) 
        dataset[field][ind] = out
        out_workspace.pop(outkey)

if debug: print loop_workspace, out_workspace
for outfile in outfiles.values():
  outfile.close()

quit()

# Now, read the data and compute the mass.
recs = [hu.record_id.flatten(), p0.record_id.flatten(), dx.record_id.flatten()]

recs += [r.flatten() for r in tracers.values()]
print recs
